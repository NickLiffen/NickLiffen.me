<!doctype html>
<html lang="en">
<head>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-ZG0XYVVEZW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-ZG0XYVVEZW');
</script>
  <meta charset="utf-8">
  <title>Nick Liffen's Blog | Technology, DevOps and Developer Blog</title>
  <meta name="description" content="Nick Liffen's Blog | Focusing on DevOps, Developer Experince and all aspects of Technology">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="author" content="Nick Liffen">
  <meta property="og:title" content="Nick Liffen's Blog | Technology, DevOps and Developer Blog">
  <meta property="og:type" content="website">
  <meta property="og:url" content="https://nickliffen.dev/">
  <meta property="og:image" content="https://nickliffen.dev/">
  <meta name="keywords" content="Nick, Liffen, JavaScript, Blog">

  <link rel="shortcut icon" href="./favicon.ico">
  <link rel="manifest" href="manifest.webmanifest" crossorigin="use-credentials">
  <link rel="apple-touch-icon" href="./icon.png">
  <link rel="alternate" type="application/rss+xml" title="Subscribe to What's New" href="http://nickliffen.dev/rss.xml" />
  <link rel="stylesheet" type="text/css" href="./css/poole.css">
  <link rel="stylesheet" type="text/css" href="./css/hyde.css">

  <meta name="theme-color" content="#fafafa">
  <script type="application/ld+json">
{
	"@context": "http://schema.org",
	"@type": "Blog",
	"@id": "https://nickliffen.dev/",
	"name": "Nick Liffen's Blog | Technology, DevOps and Developer Blog",
	"headline": "Nick Liffen's Blog | Technology, DevOps and Developer Blog",
	"description": "Nick Liffen's Blog | Focusing on DevOps, Developer Experince and all aspects of Technology",
	"datePublished": "2021-01-30",
	"dateModified": "2021-01-31",
	"keywords": ["Blog", "Technology", "DevOps", "Nick", "Liffen", "Developer", "GitHub"],
	"url": "https://nickliffen.dev/",
	"author": {
		"@type": "Person",
		"name": "Nick Liffen"
	},
	"blogPosts": [{
      "@type": "BlogPosting",
      "@id": "https://nickliffen.dev/articles/step-functions.html",
      "name": "Nick Liffen's Blog | Coordinating a multi-lambda software product | Blog Post",
      "headline": "Let's discuss the value of Step Functions by sharing a real life example.",
      "description": "Nick Liffen's Blog | Coordinating a multi-lambda software product | Blog Post",
      "image": "https://nickliffen.dev/img/ghas.png",
      "datePublished": "2021-08-29",
      "dateModified": "2021-08-29",
      "keywords": ["Nick", "Liffen", "Software", "Lambda", "Step Functions"],
      "articleSection": ["Context", "Model One - SQS", "Model One - Step Functions", "Going into Detail about Step Functions", "Conclusion"],
      "articleBody": "...",
      "url": "https://nickliffen.dev/articles/step-functions.html",
      "author": {
        "@type": "Person",
        "name": "Nick Liffen"
      },
      "publisher": {
        "@type": "Person",
        "name": "Nick Liffen"
      }  
    },
    {
			"@type": "BlogPosting",
			"@id": "https://nickliffen.dev/articles/centralised-vs-innersourcing-devops.html",
			"name": "Nick Liffen's Blog | An InnerSourcing Approach within an Enterprise | Blog Post",
			"headline": "Let's discuss the value of InnerSoucing by sharing a real life example.",
			"description": "Nick Liffen's Blog | An InnerSourcing Approach within an Enterprise from Eli Lilly on InnerSourcing | Blog Post",
			"image": "https://nickliffen.dev/img/innersourcing.png",
			"datePublished": "2021-02-13",
			"dateModified": "2021-02-13",
			"keywords": ["Nick", "Liffen", "InnerSourcing", "Blog", "Enterprise"],
			"articleSection": ["Context", "What does centralisation mean?", "What does decentralisation mean?", "So, what one should we pick?", "Conclusion"],
			"articleBody": "...",
			"url": "https://nickliffen.dev/articles/innersourcing.html",
			"author": {
				"@type": "Person",
				"name": "Nick Liffen"
			},
			"publisher": {
				"@type": "Person",
				"name": "Nick Liffen"
			}
		},
		{
			"@type": "BlogPosting",
			"@id": "https://nickliffen.dev/articles/centralised-vs-decentralised-devops.html",
			"name": "Nick Liffen's Blog | Centralised vs Decentralised DevOps | Blog Post",
			"headline": "Centralised vs Decentralised of a DevOps Toolchain within an Enterprise",
			"description": "Nick Liffen's Blog | Centralised vs Decentralised of a DevOps Toolchain within an Enterprise | Blog Post",
			"image": "https://nickliffen.dev/img/devops.png",
			"datePublished": "2021-02-07",
			"dateModified": "2021-02-307",
			"keywords": ["Nick", "Liffen", "JavaScript", "Blog", "DevOps", "Enterprise", "Centralised", "Decentralised"],
			"articleSection": ["Context", "What does centralisation mean?", "What does decentralisation mean?", "So, what one should we pick?", "Conclusion"],
			"articleBody": "To centralise or decentralise, that's the question.",
			"url": "https://nickliffen.dev/articles/centralised-vs-decentralised-devops.html",
			"author": {
				"@type": "Person",
				"name": "Nick Liffen"
			},
			"publisher": {
				"@type": "Person",
				"name": "Nick Liffen"
			}
		}
	]
}
</script>
</head>

<body>
  <div class="sidebar">
    <div class="container sidebar-sticky">
      <div class="sidebar-about">
        <h1>Nick Liffen</h1>
        <p class="lead">This is where I share my experiences</p>
      </div>
      <ul class="sidebar-nav">
        <li class="sidebar-nav-item active">
          <a href="/">Home</a>
        </li>
        <li class="sidebar-nav-item">
          <a href="https://github.com/nickliffen">GitHub Profile</a>
        </li>
      </ul>
      <p>&copy; 2021. All rights reserved.</p>
    </div>
  </div>

  <div class="content container">
    <h1 class="center post-title mb">Table of Contents</h1>
    <div class="center mb">
      <div class="margin"><a href="./articles/step-functions.html">Coordinating a multi-lambda software product</a></div>
      <div class="margin"><a href="./articles/innersourcing.html">An InnerSourcing Approach within an Enterprise</a></div>
      <div class="margin"><a href="./articles/centralised-vs-decentralised-devops.html">Centralised vs Decentralised DevOps Toolchain - The Differences</a></div>
      <div class="margin"><a href="./articles/review-ghas-code-scanning-enterprise.html">Review of GitHub Advanced Security Code Scanning in an Enterprise (Part 1)</a></div>
      <div class="margin"><a href="./articles/building-a-simple-website.html">Building a simple website</a></div>
    </div>
    <hr class="rounded">
    <div class="posts">
      <div class="post">
        <h1 class="post-title mb">
            Coordinating a multi-lambda software product
        </h1>
        <span class="post-date">29th Aug 2021</span>
        <h2 id="introduction" class="mb">Introduction:</h2>
        <p>The more I have been working on AWS, the more I understanding the importance of well-architected solutions. Today, I would like to focus on the value of <a href="https://aws.amazon.com/step-functions/?step-functions.sort-by=item.additionalFields.postDateTime&step-functions.sort-order=desc" target="_blank" rel="noreferrer">AWS Step Functions</a>. What are Step Functions? The offical description is: </p> 
        <p><em>
          AWS Step Functions is a low-code visual workflow service used to orchestrate AWS services, automate business processes, and build serverless applications. Workflows manage failures, retries, parallelization, service integrations, and observability so developers can focus on higher-value business logic.
        </em></p>
        <p>To explain the value, I am going to use some hypothetical use cases, which are:</p>
        <ul>
            <li><strong>Use Case One:</strong> As a GitHub administrator, whenever a new developer joins the GitHub organisation, I would like to add them to a GitHub Team so that they can access repositories straight away. I also would like to fetch internal company information about that user (email, id, etc.) and add them to an internal DB for querying.</li>
            <li><strong>Use Case Two:</strong> As a GitHub administrator, whenever a GitHub Workflow completes, I would like to calculate the workflow cost and work out the total workflow count for the repository, so it's easy to do chargebacks per repository. I also would like to store the data in a DB so it can be queried historically.</li>
        </ul>
        <p>Before we build out some architectures, let's set some principles:</p>
        <ul>
            <li><strong>Purposeful:</strong> Single function lambda's for single-use cases. (not combining multiple businesslogic into a single lambda). This is to promote reuse.</li>
            <li><strong>Event Driven:</strong> No polling or waiting on a cron which triggers to see if things have changed. I would like an end-to-end event driven architecture.</li>
            <li><strong>Stateful:</strong> Non-Invoke Based (Callback hell). E.G Lambda A Invoked Lambda B from within Lambda A and waits for Lambda B to be done to return success/fail</li>
        </ul>
        <p> Now, as with any architecture, there are multiple ways to build out this example system. I will show two examples below and compare & contrast the pros and cons of each, mainly focusing on how to use multiple lambdas together and why AWS Step Functions are beneficial.  </p>
        <h2 id="sqs" class="mb">Model One - SQS:</h2>
        <img src="./img/sqs.png" alt="AWS SQS Arch Design" style='height: 100%; width: 100%; object-fit: contain' loading="lazy">
        <p>Let's walk through the above diagram. We have a GitHub App configured on two events (A, B). We use a GitHub App to remove the "human" element of the connection, along with some other goodies like an increase in API Requests. The GitHub App will send a payload to our API, but before it reached the API, we use AWS Route 53 for our custom DNS record, which then will proxy down to our AWS Cloudfront Distribution. </p>
        <p>Once the payload reaches the API, we will first use the direct integration between AWS HTTP API Gateways and AWS Lambda to first process the data. Then, to communicate between the rest of the lambdas, we use AWS SQS to traffic data between lambdas for processing. Finally, data ends up in the database where you could use a service like AWS AppSync or another API Gateway to fetch the data.  </p>
        <p>Let's talk about the pros:</p>
        <ul>
            <li><strong>Extensible:</strong> It's extensible, as an individual SQS Queue fronts each function. Meaning, you're able to quickly send data to that function from any service that can send structured data. You may know about Lambda X needing to send data to Lambda Y. Still when a new Lambda comes in, Lambda Z, it's easy to add that lambda into the current architecture and send data to Lambda Y without breaking the current pattern.</li>
        </ul>
        <p>Let's talk about the cons:</p>
        <ul>
            <li><strong>Clean Arch:</strong> It's a little messy. I am a big believer that most clean architectures are simple. Don't overcomplicate something and add AWS services because it could fit a need. Look at alternatives to reduce your footprint. There are 6 SQS Queues; they seem to be the most predominant service in this design. Are they needed? </li>
            <li><strong>Problem Finding:</strong> How easy is it to <strong>really</strong> find problems? We have a dead letter queue configured so any messages that don't complete can be re-processed accordingly, but you only see a problem at a time; you don't see the history of where that data has come from or where it has been or how it has been processed. You would have to write some custom code to do this. </li>
            <li><strong>App Tracking:</strong> Amazon SQS requires you to implement application-level tracking, especially if your application uses multiple queues, which in this case, it does. </li>
        </ul>
        <p> Overall, this isn't a bad architecture, it fits a use case, but could it be fine-tuned?</p>
        <h2 id="sf" class="mb">Model Two - Step Functions</h2>
        <img src="./img/sf.png" alt="AWS Step Functions Arch Design" style='height: 100%; width: 100%; object-fit: contain' loading="lazy">
        <p>Both ingress patterns into AWS are the same. The main difference starts when you get past the AWS HTTP API Gateway and into the data processing. </p>
        <p>As this solution has multiple lambdas, we use AWS Step Functions to coordinate how they interact. So, when a payload reaches the HTTP API, we trigger the AWS Step function. Data is processed by each lambda and sent back to the state machine, where finally it inserts data into the DB and uses a custom AWS SNS Topic to send an email on success/error. </p>
        <p>Both have similar architectures but differ slightly in data communication; let's discuss the detail ... </p>
        <p>Let's talk about the pros:</p>
        <ul>
            <li><strong>Less Code:</strong> We don't have to write a custom lambda to enter data into the DB. Step functions have a native integration with DynamoDB, meaning we don't have to write code to do something preexisting. More information on integrations can be found here: <a href="https://docs.aws.amazon.com/step-functions/latest/dg/concepts-service-integrations.html" target="_blank" rel="noreferrer">Using AWS Step Functions with other services</a>.</li>
            <li><strong>Less AWS Resource(s):</strong> No need for any queues. We use the state machine to send data to the following lambda in the chain. More information on how to send data within step functions can be found here: <a href="https://docs.aws.amazon.com/step-functions/latest/dg/concepts-state-machine-data.html" target="_blank" rel="noreferrer">State Machine Data</a></li>
            <li><strong>Process Overview:</strong> Easy to see the whole process in action. Step Functions easy allow you to see the data that is processing. To see more information on seeing the overall process, check out this link: <a href="https://docs.aws.amazon.com/step-functions/latest/dg/concepts-input-output-filtering.html" target="_blank" rel="noreferrer">Input and Output Processing in Step Functions</a></li>
            <li><strong>Easy to find problems:</strong> Don't you dislike having to crawl through cloudwatch events to find errors logged out from a lambdas console? Using AWS Step Functions allows you to quickly find errors via the Step Functions GUI as you can crawl through the state machines events to find problems. I find this link really useful for more information on debugging: <a href="https://docs.aws.amazon.com/step-functions/latest/dg/monitoring-logging.html" target="_blank" rel="noreferrer">Monitoring &amp; Logging</a></li>
            <li><strong>Built in retries:</strong> Sometimes lambdas error and writes into DynamoDB's fail. Although they are rare, if not handled correctly, they could cause downstream dilemmas. Step Functions have inbuilt retry capabilities that allow you to retry on specific errors. Meaning you can only retry on specific event errors that you would like to retry on. More information on this can be found here: <a href="https://aws.amazon.com/blogs/developer/handling-errors-retries-and-adding-alerting-to-step-function-state-machine-executions/" target="_blank" rel="noreferrer">Monitoring &amp; Logging</a></li>
        </ul>
        <p>Let's talk about the cons:</p>
        <ul>
            <li><strong>Con One:</strong> Step Functions has some pretty strict and small <a href="https://docs.aws.amazon.com/step-functions/latest/dg/limits-overview.html" target="_blank" rel="noreferrer">limits</a> (I actually think this article is a nice summary of the limits: <a href="https://docs.aws.amazon.com/step-functions/latest/dg/monitoring-logging.html" target="_blank" rel="noreferrer">Think Twice Before Using Step Functions — Check the AWS Serverless Service Quotas</a>). If you are processing lots of data, you would need to split your step functions into multiple state machines. One idea on how to architect your solution around this limit is to create a parent/child sate machine. E.G, a child state machine could process a single data entry at a time, which invokes from a parent state machine that loops through the data, but doesn't directly <strong>do</strong> any of the processing, so it stays within limits. </li>
            <li><strong>Con Two:</strong> If another system needs to reuse a specific function, there is no queue in front of it, making it harder to call. Making the architecture not amazingly extensible. Yes, you can still use AWS SQS with step functions, but unless you think it's needed externally to this use case, it likely isn't required. </li>
        </ul>
        <p> Overall, I genuinely believe this architecture is cleaner and runs a more robust process than the previous design. </p>
        <h2 id="sf-detail" class="mb">Going into detail about Step Functions</h2>
        <p>I would like to focus on two core parts of step functions that stand out to me:</p>
        <h3>Feature One: Built in looping through arrays</h3>
        <p>Let's say you have a data set of 1,000 users. You <i>could</i> send all 1,000 users to the lambda via an SQS Queue (but you can only send ten records at a time, remember), loop through all users, process them accordingly and send them back to the state machine. Or, you could use the inbuilt <a href="https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-map-state.html" target="_blank" rel="noreferrer">map</a> feature within Step Functions that will map through the user's array at the sate machine level and send one user record at a time to the lambda for processing. Why would you do this? It allows you to write less code within your lambda, fewer loops, hopefully, quicker processing. In my opinion, it also makes your code cleaner. </p>
        <p>It looks a little like this within the state machine definition file:</p>
        <pre>
Invoke Worker State Machine:
Type: Map
InputPath: "$.users"
MaxConcurrency: 50
Parameters:
    UserDetails.$: "$$.Map.Item.Value"
        </pre>
        <h3>Feature Two: AWS Step Functions can call AWS Step Functions</h3>
        <p>As mentioned above, AWS Step Functions have some pretty strict (and small) limits. Meaning you have to architect your solutions correctly. An elegant aspect of Step Functions is they can call other <a href="https://docs.aws.amazon.com/step-functions/latest/dg/sample-start-workflow.html" target="_blank" rel="noreferrer">step functions</a>. Meaning if you have been processing lots of data and are reaching limits, you can split up your Step Function into one parent step function, and then a child step function where you send one data record at a time to be processed individually from the parent.</p>
        <p>It looks a little like this within the state machine definition file:</p>
        <pre>
Iterator:
StartAt: Invoke Worker State Machine Task
States:
    Invoke Worker State Machine Task:
    Type: Task
    Resource: arn:aws:states:::states:startExecution.sync:2
    Parameters:
        StateMachineArn: "${ChildStateMachineArn}"
        Input:
        UserDetails.$: "$.UserDetails"
        AWS_STEP_FUNCTIONS_STARTED_BY_EXECUTION_ID.$: "$$.Ex.Id"
        </pre>
        <p>Taking the above two code snippets, you are looping through the user's array of objects and sending one user at a time. Let's say you have 1,000 users; you are spinning up 1,000 child step functions and processing one user at a time. </p>
        <p> These are just two features that I think make Step Functions a great resource to use when co-ordinating a multi-lambda solution. However, there are so many more, check out the following docs for more information <a href="https://docs.aws.amazon.com/step-functions/latest/dg/welcome.html" target="_blank" rel="noreferrer">Introduction to AWS Step Functions</a></p>
        <h2 id="conclusion" class="mb">Conclusion</h2>
        <p>I have found Step Functions to be a great resource when working across lambdas. They give you more confidence in your design and allow you to write less code, and in most cases, less code is better code, right? </p>
      </div>
      <hr class="rounded">
      <div class="post">
        <h1 class="post-title mb">
          An InnerSourcing Approach within an Enterprise
        </h1>
        <span class="post-date">14th Feb 2021</span>
        <h2 id="context" class="mb">Context:</h2>
        <p>InnerSourcing. An approach that could revolutionise software development within enterprises, yet an approach unknown to most. So, what is InnerSourcing? Most people/companies have different interpretations, but overall, InnerSourcing takes the values and principles of "open source" and looks to apply that within an enterprise. Traditionally, software teams are siloed and have their objectives, meaning they are focused on their deliverables and nothing else. Typically, if these teams were to take a step back and look across the enterprise, they would see other teams trying to do the same thing, or even better, other teams have solved/come up with a solution already. What InnerSourcing tries to achieve is collaboration across teams to meet a common goal. Why have multiple teams work on the same thing separately, versus combining everyone's knowledge in these teams to work together to meet the outcome as one. </p>
        <h2 id="advantages" class="mb">Advantages:</h2>
        <p>InnerSourcing has some more values than just cross-team collaboration; some more advantages are:</p>
        <ul>
          <li><strong>Faster development:</strong> Having multiple developers from various teams will lead to faster development, versus a single team where only one or two developers may be focused on that deliverable.</li>
          <li><strong>Documentation:</strong> If multiple teams are consuming a software product, more focus is put on writing good documentation. This is so developers looking to use the software product can get up and running quickly, rather than figure it out themselves.</li>
          <li><strong>Code Reuse:</strong> It is better to build once and reuse multiple times, then build several times and use once. This is because code is more maintainable as if an update needs to be made, it gets made once, versus every team needing to make the same change.</li>
          <li><strong>Encourage innovation:</strong> Innersourcing gets more minds on the same outcome. Naturally, this means a more diverse mindset, leading to a broader range of solutions being proposed to a problem; therefore, you encourage innovation. </li>
          <li><strong>Healthy Competition:</strong> The more advanced companies create healthy competition by rewarding developers who have "InnerSourced" the most. Doing this gets the most out of your developers and keeps them engaged.</li>
          <li><strong>Security &amp; Quality:</strong> This is one of the more unrecognised values. Building something once and reusing it multiple times means if an urgent security vulnerability needs to be patched, the update just needs to be made to a single codebase. The teams using the fixed software need to do nothing. On the next release of their software, it will get automatically pulled down and patched.</li>
        </ul>
        <p>These are just a few examples to show why InnerSourcing can be such a success within enterprises. </p>
        <h2 id="success-story" class="mb">Success Story</h2>
        <p>At this point (traditionally) in my blog, I would usually explain how something works and share my experience/further thoughts on the topic. However, for this article, I would like to link out to a conference I spoke at back in 2018.</p>
        <p>Every year, GitHub host a conference in San Francisco called "GitHub Universe". This conference focuses on all aspects of software development, where GitHub and companies present approaches and best practices. I was lucky enough to present on behalf of Lilly, to talk about Lillys's InnerSourcing journey and the value seen internally. I discuss the concepts of InnerSourcing, the approach taken, some statistics of adoption, then ending with some best practices and tips &amp; tricks for other companies looking to adopt the same.</p>
        <p>See the video below:</p>
        <iframe class="video" src="https://www.youtube.com/embed/L19lgbIPQZs" title="Lilly's InnerSourcing Journey" loading="lazy" allowfullscreen="true"></iframe>
        <h2 id="conclusion" class="mb">Conclusion</h2>
        <p>To conclude, similar to DevOps, InnerSourcing can be seen as an approach and methodology to transform IT enterprises' traditional culture and behaviours. It helps to have a central team spearheading the journey, as that team can focus on automating the process which sparks and enables InnerSourcing. Additionally, it also helps to share and use the same suite of DevOps Tools (such as source code and package manager tools), as this removes friction in the developer experience. Asking a developer to go between multiple tools will lead to developer frustration and therefore, a lower appetite to adopt InnerSourcing. Developer experience has to be at the heart and focal point of a companies InnerSourcing journey.</p>
        <p>DevOps and InnerSourcing, although different, complement each other more then people realise. If companies lay solid foundations for one, it will naturally help lay the other's foundations, even if unintentional. Finally, I believe InnerSourcing will soon be the "norm" and a core approach for how software development occurs within enterprises. I am excited to see InnerSourcing grow and mature through various companies in the coming years. </p>
      </div>
      <hr class="rounded">
      <div class="post">
        <h1 class="post-title mb">
          <a href="./articles/centralised-vs-decentralised-devops.html"> Centralised vs Decentralised DevOps Toolchain - The Differences </a>
        </h1>
        <span class="post-date">7th Feb 2021</span>
        <h2 id="context" class="mb">Context:</h2>
        <p>To centralise or decentralise, that's the question. When enterprises adopt DevOps, one of the most important considerations is how internal teams consume a toolchain supporting the DevOps mindset.</p>
        <p>In this article, I want to focus on the big question around how DevOps should (or could) be delivered within an enterprise. Should companies enable complete freedom and decentralisation of their toolchain and process? This lets teams put their own flavour on the core principles of DevOps. Or, do companies go for a more centralised route? This means more standardisation &amp; consistency across the enterprise and an easier adoption avenue due to central capabilities. Let's discuss the pros and cons of each and come up with a recommendation.</p>
        <h2 id="what-does-centralisation-mean" class="mb">What does centralisation mean?</h2>
        <p>Centralisation of DevOps tooling means offering a single toolchain and process for teams to follow across an enterprise. When you centralise, you provide multiple teams with a single avenue to adopt a DevOps mindset. There are many advantages to centralisation, some examples being:</p>
        <ul>
          <li><strong>Adoption:</strong> Whenever you centralise, you usually have a higher adoption percentage off the bat. This is mainly due to teams who are new to DevOps, more specifically, the tools that come as part of the DevOps toolchain, have a place to start and consume something <i>as a Service</i>. If you rely on teams standing up their own tooling, the time to adoption is generally longer as there is more setup time.</li>
          <li><strong>Consistency:</strong> If you centralise a process, an expected outcome of that is consistency. This means that if a developer is working on team X, and then moves to team Y; there is less "getting up to speed" time, as the processes would be the same for Team X and Team Y. This is a massive win if your company promotes rotation of developers and engineers across teams.</li>
          <li><strong>Time to Value:</strong> Application teams new to DevOps will find it quicker and easier to deploy something to production if you centralise. Teams spread across an enterprise can always ask questions to the central team owning DevOps, so someone is around to help out and ask questions. There is short term success here and a quicker time to value.</li>
        </ul>
        <p>As you can tell, there is a theme to the above. The newer your enterprise is to DevOps, the more value you will see from centralisation. However, there are some disadvantages or considerations to keep in mind.</p>
        <ul>
          <li><strong>Slow turnaround of Change:</strong> If only one team are allowed to own and maintain tools, and a customer puts in a feature request, that request may be one request often already in the backlog. As that customer who logged that bug, you may have to wait a month, even longer to see that feature request in production. Whereas if you had control of the tools in your application DevOps toolchain, you make the decisions, so the power is in your hands.</li>
          <li><strong>Reliance &amp; Excuses:</strong> If you centralise, it is easy for application teams to rely heavily on the team's who own the central tools. Especially if there is something the application team is waiting on. There is an excuse to why they are behind or not moving at the pace they should be, "I can't do this, just waiting for the central team to deliver something". Too much reliance on one team means you don't move at the pace that you may want to move at.</li>
        </ul>
        <p>There are lots of advantages to centralisation, especially for larger & legacy enterprises. Now let us discuss what decentralisation means.</p>
        <h2 id="what-does-decentralisation-mean" class="mb">What does decentralisation mean?</h2>
        <p>Decentralisation of DevOps tooling is the opposite of centralisation. It's where there is not central tooling, there is no central process, and you allow your enterprise to embrace the principles of DevOps themselves. There is a trust that they will see the value in DevOps, and have the creative and learning agile mindset to read, up-skill and apply. Some advantages of decentralisation are:</p>
        <ul>
          <li><strong>Embracing Diverse DevOps Models:</strong> There isn't one way to adopt DevOps, and there most certainly isn't one way to use a DevOps Toolchain. If you centralise too much, you will find people "doing the same thing" and not getting the most out of the tools that they are using.</li>
          <li><strong>Learning Agility:</strong> The value of decentralisation is that you are equipping your enterprise for learning success. Change has been, and always will be happening, especially within IT. If you fully decentralise, you are asking and almost enforcing your enterprise to upskill themselves. What this leads to is a longer-term success. If employees are used to changing and are used to learning new tools &amp; processes, anything new that comes up in the future should require less time to adopt as employees are used to a changing environment.</li>
          <li><strong>Innovation:</strong> This is a pivotal point to understand. You want your employees to be innovative and apply learnings to their working model. For some reason, there is a preconception that the centralised team knows best and what they say should go. This is not always the case. Just because the central team think they know what's right for the enterprise, doesn't mean they do. If you decentralise your DevOps process and toolchain, you are letting your employees innovate and think differently without the central team's restrictions. A different way of thinking about this is - would you rather a central team of 40 people making decisions, or would you like 1000 people (or everyone in your company) learning and deciding what's best. One thousand minds are better than 40.</li>
        </ul>
        <p>However, for all these advantages, there are possible disadvantages to think about:</p>
        <ul>
          <li><strong>Can be slow to get enterprise-wide adoption:</strong> Normally, leadership within an enterprise want quick results and want to see success straight away. If you have a highly skilled and learning agile company, this may not be so much of a problem. But companies who have been around a while may struggle with getting results quickly.</li>
          <li><strong>You may not see shared learnings:</strong> There is a reality to decentralisation: some teams will get it quicker than others. As there is no central team, there may not be an easy way to share learnings across teams. This will lead to some teams racing ahead of others, with the high likelihood some teams will never get it and will be left behind. At the end of the day you are one company, so the success shouldn't be perceived as team success, it should be company success. </li>
        </ul>
        <p>Similar to the centralisation section, there is a theme appearing. A newer company or a company with a highly agile culture is likely going to suit decentralisation more. I believe following this model will set yourself up for success longer-term; when done right.</p>
        <h2 id="what-one-should-we-pick" class="mb">So, what one should we pick?</h2>
        <p>The reality of this question is that there is no right answer. But there is a recommendation: </p>
        <p class="center"><strong>Centralise the DevOps Toolchain's foundations, whilst decentralising the journey on "how" teams adopt &amp; consume the centralised tools, patterns, and processes.</strong></p>
        <p>So, what does this mean? Let's break down that sentence into two:</p>
        <p class="center"><strong>"Centralise the DevOps Toolchain's foundations".</strong></p>
        <p>It isn't time effective, or cost-effective to have multiple of the same tools, or similar tools spread across an enterprise. An example being source code management. Stand up a single source code management tool and offer it out to the enterprise <i>as a Service</i>. This will reduce operational overhead because instead of multiple teams managing that tool; it will be supported and maintained centrally. Additionally, it's typically cheaper contractually if multiple licences are purchased centrally vs distributed across numerous contracts. For centralisation to be a success, automation and end-user autonomy needs to be a core principle. Staying with the source code management example mentioned above, you should allow customers to create repositories themselves without putting a request in and waiting. Allow teams to create teams themselves, as well as administer their own repository. Ensure there is no friction in the way of the customer of the tool. The more "red tape" and "friction" in the way of the customer and the centralised tool, the less chance of success. If you're thinking, "I can't give administration access and full autonomy in my company, we have quality process and regulations we need to meet." Just because you give autonomy and allow self-service, doesn't mean you can't wrap process and rigour around the tool, ensuring quality and security compliance. Use API's to build the needed quality processes to ensure the centralised tool is "in compliance" of any in-company procedures. Then publish documentation of any processes, so customers know what to expect. Mimic this for every tool within your toolchain.</p>
        <p>Additionally, understand that you will have different teams at different skill levels, so centralising foundational blueprints and patterns for teams to get started with makes sense. For example, within GitHub, create <a href="https://docs.github.com/en/github/creating-cloning-and-archiving-repositories/creating-a-template-repository">repository templates</a> which follow best practice (locked branches, pull requests enforced, turned on security tools). This means teams new to GitHub have a place to start "out of the box" with everything needed pre-created. The same goes for your CI/CD tool. Build reusable workflows/pipelines with standard testing &amp; deployment patterns across your enterprise. For example, you may be a predominately JavaScript shop, so building a single pipeline that does JavaScript testing/linting/security means teams new to CI/CD have a place to start (additionally teams with CI/CD experience may not want to build their own so will reuse what is on offer centrally). Thirdly, and finally, is the hosting platform chosen (AWS, Azure, GCP). Build central patterns for common software application architectures. e.g. if many of your software products built are web assets, creating a SPA template that comes with the needed infrastructure as code to deploy to AWS S3 & Cloud-front. This means teams have a place to start, especially people new to cloud-native. The same for API's would apply, an OpenAPI template that comes with AWS API Gateway & Lambda would be a great starter kit. </p>
        <p>Okay, so now you may be thinking wow this is a lot of centralisation, let's shift to the second part of the above phrase which is:</p>
        <p class="center"><strong>"Whilst decentralising the journey on "how" teams adopt &amp; consume the centralised tools, patterns, and processes." </strong></p>
        <p>You are not the police. You cannot dictate and enforce how teams use your tools. If you lock everything down to a single way of doing things, you will slow down teams. This will fail. Your company may have adopted a "DevOps Toolchain", but your teams won't be moving fast and won't be adopting the principles of DevOps. Past standing up the tools and foundational patterns/starter kits, let teams decide how they want to use the tools you have stood up.</p>
        <p>An example being your hosting platform. Stand up an underlying hosting service that meets the needed quality &amp; security processes for the enterprise, but then get out the application teams way. Do not centralise manual gates and reviews to one team as that team "they know best". </p>
        <p>Doing this will massively slow down not just deployment time, but learning time. Give autonomy for teams to deploy from Dev -> Q.A. -> Prod. If teams fail, that's okay, let them learn from that failure and apply that learning for next time. </p>
        <p>The same goes for CI/CD. In the above paragraph, I explained building example pipelines/workflows. Do not dictate this is the only way for teams to use CI/CD. Allow teams to see these as examples, and then customise to suit their needs. Teams may not even use these pre-built pipelines/workflows, and that's okay. They are there for reference and use if desired.</p>
        <p>The main point to be conveyed here is that there is no one way of doing things. Do not centralise the process, just the foundations and tools. Doing this will set teams up for success by themselves. Additionally, the more centralisation you do and the more is done for teams, the less they will learn themselves. This puts enterprises at a further disadvantage because if centralised tools change, or there is an update in a procedure that requires a change on the application team, teams will be foreign to doing things themselves, leading to slower change adoption.</p>
        <h2 id="conclusion" class="mb">Conclusion</h2>
        <p>There is a misconception that DevOps should be a specific role, and only people who's job title includes "DevOps" needs to focus on DevOps. In my opinion, this isn't, and shouldn't be the case. DevOps is a mindset (process) which teams need to adopt when building and deploying software products. Everyone part of that software team somewhat contributes to the DevOps process. As mentioned above, there is no right answer when it comes to centralisation or decentralisation, it depends on the company and most importantly, the people. In most enterprises, a mixture, or somewhere in-between centralisation/decentralisation, will give the best experience for different teams. To conclude, centralisation will get you quicker success but can be short term thinking. Decentralisation will take longer to get there but in the long term can see more significant benefits.</p>
        <p>The key takeaway though is centralisation works, but only when done at the correct level, and autonomy is vital.</p>
      </div>
    </div>
    <div class="pagination">
      <span class="pagination-item older">Older</span>
      <a href ="/articles/page/2.html"><span class="pagination-item newer">Newer</span></a>
    </div>
    </div>
<script>
if ('serviceWorker' in navigator) {
 navigator.serviceWorker.register('/sw.js').then(function() {
   console.log('service worker is is all cool.');
 }).catch(function(e) {
   console.error('service worker is not so cool.', e);
   throw e;
 });
}
</script>
</body>

</html>
